---
title: "Project Report - Code"
author: "Xinyu Dong", "CHEN ZIYING(Sophie)", "Yau Matthew"    
date: "2023-03-17"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Dataset Introduction and 
There are 14 categorical and 4 numerical variables in the dataset, and our target variable is “HeartDisease”. This is a clean dataset without any missing data. Among the 319,795 observations, we removed 18,078 duplicates. Therefore, the following explanatory data analysis would only perform on 301,717 observations.


## Data Processed
```{r}
library(dplyr)

data <- read.csv("heart_2020_cleaned.csv")
# head(data)

heart_2020 <- data[!duplicated(data), ]  # remove duplicate data
heart_2020$Race[heart_2020$Race != "White"] <- "non-white" # make race binary 

heart_2020 %>% mutate_if(is.character, as.factor) -> heart_2020
```


## Decision Tree
Through the explanatory analysis, we found that our target variable *HeartDisease* is unbalanced which most of the cases do not have heart disease.

### Fiting and Tunning

```{r,fig.align = 'center',out.width="50%"}
# Load data
library(readr)

library(tree)


# Fit decision tree model
model.trees <- tree(HeartDisease ~ ., data = heart_2020)


heart.treecv <- cv.tree(model.trees) # Cross-validation
plot(heart.treecv) 

# Tuning
heart.tree.prune <- prune.misclass(model.trees, best=5)
plot(heart.tree.prune)
text(heart.tree.prune,pretty = 0)

```

This is a seemingly strange but interesting decision tree. We can see from the results of the decision tree that the most certain thing about the decision tree is that if you are **under 54 years** of age and **in good overall health**, then the decision tree will assume that you will not develop heart disease. Other factors such as mental health, race, physical activity, etc., have little impact on whether or not you will develop heart disease if you meet the age and overall health criteria.

Now we have identified the low risk group: *they are under 45 years of age and their overall health is good or above*. So let's go a step further and identify those who are not in this range. We will remove the people who meet the age under 45 and overall overall health. The conclusion is that the decision tree is still trying to identify people who do not have heart disease by their age and overall health status. Therefore, we directly remove the union set that satisfies both categories.

```{r,fig.align = 'center',out.width="50%"}
obs_not_low_risk <- subset(heart_2020,!(AgeCategory %in% c('18-24',
                                                           '25-29',
                                                     '30-34',
                                                     '35-39',
                                                     '40-44',
                                                     '45-49',
                                                     '50-54') |
                         
                                 GenHealth %in% c('Good','Very good','Excellent'))
                         )

#New data Investigation 
dim(obs_not_low_risk)
nrow(obs_not_low_risk)/nrow(heart_2020)
prop.table(table(obs_not_low_risk$HeartDisease))
prop.table(table(heart_2020$HeartDisease))

```

After removing those who passed age threshold and were in good overall health, we selected a total sample of 31,857 (only 9.96% of the total sample). The proportion of people suffering from heart disease in this sample rose to 29.76%. Compared to only 8.6% for the entire sample frame, it is already a significant and encouraging improvement - don't forget that we are only referring to the simple conditions of age 45+ and overall health below health.

```{r,fig.align = 'center',out.width="50%"}
# Fit decision tree model
model.trees <- tree(HeartDisease ~ ., data = obs_not_low_risk)


heart.treecv <- cv.tree(model.trees) # Cross-validation
plot(heart.treecv) 

# Tuning
heart.tree.prune <- prune.misclass(model.trees, best=3)
plot(heart.tree.prune)
text(heart.tree.prune,pretty = 0)


Stroked <- subset(obs_not_low_risk,Stroke == 'Yes')

#New data Investigation 
dim(Stroked)
nrow(Stroked)/nrow(heart_2020)
prop.table(table(Stroked$HeartDisease))
prop.table(table(heart_2020$HeartDisease))
```

Taking the screened non-low-risk people to the next step of decision tree regression, we identified another important signal: **whether or not the person had a stroke.** If there was no stroke the decision tree would assume that the person would not have had a heart attack. In fact if we pick out the people who are already in our risk population who also had a stroke, we can see that the risk of having heart disease if they had a stroke increased from 29.76 to 49.65%. This is also a significant increase. Such a result is not difficult to explain. Stroke is often associated with hardening and blockage of blood vessels, and this often indicates that the patient has a worse blood circulation, which is also an indicator of heart disease. Now that we have greatly identified our high-risk group by age, overall health status, and whether or not we have had a stroke. Let's go one step further and see if there are other factors that can help us determine this. We'll use the data from the further targeted high-risk group in a decision tree regression.\

```{r,fig.align = 'center',out.width="50%"}


# Fit decision tree model
model.trees <- tree(HeartDisease ~ ., data = Stroked)

# Tuning
#heart.treecv <- cv.tree(model.trees) # Cross-validation
#plot(heart.treecv) 


plot(model.trees)
text(model.trees,pretty = 0)



tapply(Stroked$HeartDisease, Stroked$Sex, function(x) prop.table(table(x)))

```

We were given the simplest decision tree, whether it was male or female. What it tells us is that for these people who are more prone to heart disease, men are at higher risk than women (56.54 for men and 43.72% for women).

The ramification plot is

```{r,fig.align = 'center',out.width="50%"}
# To be ploted 
probability <- c(0.2976 ,0.4965,0.5654)

```

![](PNG%20image.png)

### Model Interpreatation

To be organized

## logistic regression
Logistic regression is a statistical method used to analyze and model relationships between a binary dependent variable (i.e., one that takes on only two values, such as 0 or 1) and one or more independent variables (also known as predictors or explanatory variables). It is a type of regression analysis that is used to predict the probability of an event occurring based on the values of the independent variables.

### Fiting and Tunning



```{r}

heart.logistic <- glm(HeartDisease~.,data = heart_2020,family = 'binomial')

summary(heart.logistic)
stepwise_model <- step(heart.logistic, direction = "both", k = log(nrow(heart_2020)), trace = 0)

summary(stepwise_model)

# calculate R^2
nullmod <- glm(HeartDisease~1, data = heart_2020, family="binomial")

1-logLik(heart.logistic)/logLik(nullmod)
1-logLik(stepwise_model)/logLik(nullmod)
```
### Add interaction terms
The R squared of these two are similar and both poor, so we decided to try adding interaction terms to see if we can predict heart disease better. Interaction term shows that one's effect on response variable depends on the other, with only one of the variable might not have predictive power, but combine them together, we can predict. In case other predictors depends on *PhysicalActivity* (which removed by stepwise process), we keep it to build the model with all interaction terms.
```{r}
formula <- HeartDisease ~ BMI * Smoking * AlcoholDrinking * Stroke * MentalHealth * DiffWalking * Sex * AgeCategory * 
    Race * Diabetic * GenHealth * SleepTime * Asthma * KidneyDisease * SkinCancer * PhysicalActivity

heart_logistic_interaction <- glm(formula = formula, data = heart_2020, family = 'binomial')
summary(heart_logistic_interaction)
1-logLik(heart_logistic_interaction)/logLik(nullmod)
```


### Analysis of the model
After stepwise selection, the predictors *PhysicalActivity* was removed from the heart.logistic model, we can apply F-test to see if the reduced model is statistically better. Since the p-value 0.0007 is pretty small at 0.05 significant level, hence, the reduced model is significantly better than the full model.

```{r}
anova(stepwise_model, heart.logistic,test = "Chisq")
1 - pchisq(14.476,2) # 1.5329 is the deviance difference
stepwise_model$formula
```

### Model Checking
Applying bootstrap to check the model, each term in the log-likelihood sum should be reasonably large in the model. Any terms that are very small indicates the failure to be modeled properly.

```{r}
library(boot)  
set.seed(92573948)
logit_test <- function(data,indices) {  
d <- data[indices,]  
fit <- glm(HeartDisease ~ BMI + Smoking + AlcoholDrinking + Stroke + MentalHealth + 
    DiffWalking + Sex + AgeCategory + Race + Diabetic + GenHealth + 
    SleepTime + Asthma + KidneyDisease + SkinCancer, data = d, family = "binomial")  
return(coef(fit))  
}
boot_fit <- boot(  
   data = heart_2020, 
   statistic = logit_test, 
   R = 100
) 
```

```{r}
boot_fit
plot(boot_fit, index=2)
```




- Based on this model we can hava a quantitative understanding of this model. Considering the AgeCategory which is considered as the most dominant factor in our decision tree. Their coeffiecnts can be show as 
```{r}
stepwise_model$coefficients
```


```{r}
library(ggplot2)
coef <- c(0.1270495,
         0.4866544,
         0.5948967,
         0.9958578,
         1.3184067,
         1.7268540,
         1.9641295,
         2.2261981,
         2.4682742,
         2.7536005,
         2.9551423,
         3.2117057)



coef_sd <- c(
  0.1241807,
  0.1110910,
  0.1063721,
  0.1000683,
  0.0964936,
  0.0931507,
  0.0916889,
  0.0908493,
  0.0905748,
  0.0905089,
  0.0910431,
  0.0907736
)


ageCategroy <- c( '25-29',
'30-34',
'35-39',
'40-44',
'45-49',
'50-54',
'55-59',
'60-64',
'65-69',
'70-74',
'75-79',
'80 or older')


coefframe <- data.frame(ageCategroy,coef,coef_sd)

ggplot(coefframe, aes(ageCategroy, coef)) +
  geom_errorbar(aes(ymin = coef - coef_sd, ymax = coef + coef_sd), width = 0.2) +
  geom_point() +
  labs(title = "Coeffients Graph of Age Category and Standard Deviation") +
  xlab("Age Category") +
  ylab("Coefficients Value")+
  theme_bw()

```


### Model Interpreatation
